{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff1e5f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Suleyman\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Suleyman\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Suleyman\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Suleyman\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Suleyman\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Suleyman\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2a44a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"dt.txt\", \"r\", encoding = \"utf8\")\n",
    "lines = []\n",
    "\n",
    "for i in file:\n",
    "    lines.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8737817b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"\"\n",
    "\n",
    "for i in lines:\n",
    "    data = ' '. join(lines)\n",
    "    \n",
    "data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce0b72df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) #map punctuation to space\n",
    "new_data = data.translate(translator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a98eadae",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = []\n",
    "\n",
    "for i in data.split():\n",
    "    if i not in z:\n",
    "        z.append(i)\n",
    "        \n",
    "data = ' '.join(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b3d633c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([data])\n",
    "\n",
    "pickle.dump(tokenizer, open('tokenizer1.pkl', 'wb'))\n",
    "\n",
    "sequence_data = tokenizer.texts_to_sequences([data])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b964af3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5642e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = []\n",
    "\n",
    "for i in range(1, len(sequence_data)):\n",
    "    words = sequence_data[i-1:i+1]\n",
    "    sequences.append(words)\n",
    "    \n",
    "sequences = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15beda3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in sequences:\n",
    "    X.append(i[0])\n",
    "    y.append(i[1])\n",
    "    \n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fce7c22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(y, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb181e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Suleyman\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\Suleyman\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=1))\n",
    "model.add(LSTM(1000, return_sequences=True))\n",
    "model.add(LSTM(1000))\n",
    "model.add(Dense(1000, activation=\"relu\"))\n",
    "model.add(Dense(vocab_size, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "289f7ced",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "keras.utils.plot_model(model, to_file='model.png', show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07527aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.python.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"nextword1.h5\", monitor='loss', verbose=1,\n",
    "    save_best_only=True, mode='auto')\n",
    "\n",
    "reduce = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.0001, verbose = 1)\n",
    "\n",
    "logdir='logsnextword1'\n",
    "tensorboard_Visualization = TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8e0d624",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0205b71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Suleyman\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 6.6950\n",
      "Epoch 00001: loss improved from inf to 6.69535, saving model to nextword1.h5\n",
      "899/899 [==============================] - 9s 10ms/sample - loss: 6.6953\n",
      "Epoch 2/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 6.6909\n",
      "Epoch 00002: loss improved from 6.69535 to 6.69113, saving model to nextword1.h5\n",
      "899/899 [==============================] - 1s 2ms/sample - loss: 6.6911\n",
      "Epoch 3/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 6.6904\n",
      "Epoch 00003: loss improved from 6.69113 to 6.69055, saving model to nextword1.h5\n",
      "899/899 [==============================] - 1s 1ms/sample - loss: 6.6905\n",
      "Epoch 4/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 6.6896- ETA: 0s - loss: 6.689\n",
      "Epoch 00004: loss improved from 6.69055 to 6.68946, saving model to nextword1.h5\n",
      "899/899 [==============================] - 1s 1ms/sample - loss: 6.6895\n",
      "Epoch 5/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 6.6876\n",
      "Epoch 00005: loss improved from 6.68946 to 6.68777, saving model to nextword1.h5\n",
      "899/899 [==============================] - 1s 1ms/sample - loss: 6.6878\n",
      "Epoch 6/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 6.6704\n",
      "Epoch 00006: loss improved from 6.68777 to 6.67455, saving model to nextword1.h5\n",
      "899/899 [==============================] - 1s 2ms/sample - loss: 6.6745\n",
      "Epoch 7/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 6.5316\n",
      "Epoch 00007: loss improved from 6.67455 to 6.53970, saving model to nextword1.h5\n",
      "899/899 [==============================] - 1s 2ms/sample - loss: 6.5397\n",
      "Epoch 8/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 6.3730\n",
      "Epoch 00008: loss improved from 6.53970 to 6.37662, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 6.3766\n",
      "Epoch 9/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 6.1532\n",
      "Epoch 00009: loss improved from 6.37662 to 6.15438, saving model to nextword1.h5\n",
      "899/899 [==============================] - 3s 3ms/sample - loss: 6.1544\n",
      "Epoch 10/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 6.0369\n",
      "Epoch 00010: loss improved from 6.15438 to 6.04795, saving model to nextword1.h5\n",
      "899/899 [==============================] - 1s 2ms/sample - loss: 6.0479\n",
      "Epoch 11/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 5.965 - ETA: 0s - loss: 5.9510\n",
      "Epoch 00011: loss improved from 6.04795 to 5.96171, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 5.9617\n",
      "Epoch 12/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 5.9039\n",
      "Epoch 00012: loss improved from 5.96171 to 5.91389, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 5.9139\n",
      "Epoch 13/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 5.8401\n",
      "Epoch 00013: loss improved from 5.91389 to 5.84908, saving model to nextword1.h5\n",
      "899/899 [==============================] - 1s 1ms/sample - loss: 5.8491\n",
      "Epoch 14/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 5.7403\n",
      "Epoch 00014: loss improved from 5.84908 to 5.76138, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 5.7614\n",
      "Epoch 15/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 5.6285\n",
      "Epoch 00015: loss improved from 5.76138 to 5.63552, saving model to nextword1.h5\n",
      "899/899 [==============================] - 1s 2ms/sample - loss: 5.6355\n",
      "Epoch 16/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 5.5611\n",
      "Epoch 00016: loss improved from 5.63552 to 5.58907, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 5.5891\n",
      "Epoch 17/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 5.4412- ETA: 0s - loss: 5.36\n",
      "Epoch 00017: loss improved from 5.58907 to 5.45727, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 3ms/sample - loss: 5.4573\n",
      "Epoch 18/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 5.261 - ETA: 0s - loss: 5.3068\n",
      "Epoch 00018: loss improved from 5.45727 to 5.33009, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 5.3301\n",
      "Epoch 19/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 5.2314\n",
      "Epoch 00019: loss improved from 5.33009 to 5.25876, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 5.2588\n",
      "Epoch 20/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 5.1913\n",
      "Epoch 00020: loss improved from 5.25876 to 5.21735, saving model to nextword1.h5\n",
      "899/899 [==============================] - 1s 2ms/sample - loss: 5.2174\n",
      "Epoch 21/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 5.1049\n",
      "Epoch 00021: loss improved from 5.21735 to 5.12631, saving model to nextword1.h5\n",
      "899/899 [==============================] - 1s 901us/sample - loss: 5.1263\n",
      "Epoch 22/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 5.0589\n",
      "Epoch 00022: loss improved from 5.12631 to 5.08840, saving model to nextword1.h5\n",
      "899/899 [==============================] - 1s 858us/sample - loss: 5.0884\n",
      "Epoch 23/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 5.0027\n",
      "Epoch 00023: loss improved from 5.08840 to 5.01941, saving model to nextword1.h5\n",
      "899/899 [==============================] - 1s 1ms/sample - loss: 5.0194\n",
      "Epoch 24/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 4.9581\n",
      "Epoch 00024: loss improved from 5.01941 to 4.98176, saving model to nextword1.h5\n",
      "899/899 [==============================] - 1s 1ms/sample - loss: 4.9818\n",
      "Epoch 25/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 4.9288\n",
      "Epoch 00025: loss improved from 4.98176 to 4.94959, saving model to nextword1.h5\n",
      "899/899 [==============================] - 1s 1ms/sample - loss: 4.9496\n",
      "Epoch 26/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 4.8176- ETA: 0s - loss: 4.72\n",
      "Epoch 00026: loss improved from 4.94959 to 4.84201, saving model to nextword1.h5\n",
      "899/899 [==============================] - 1s 1ms/sample - loss: 4.8420\n",
      "Epoch 27/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 4.6890\n",
      "Epoch 00027: loss improved from 4.84201 to 4.70105, saving model to nextword1.h5\n",
      "899/899 [==============================] - 1s 1ms/sample - loss: 4.7011\n",
      "Epoch 28/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 4.4379\n",
      "Epoch 00028: loss improved from 4.70105 to 4.46495, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 4.4649\n",
      "Epoch 29/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 4.1891\n",
      "Epoch 00029: loss improved from 4.46495 to 4.22442, saving model to nextword1.h5\n",
      "899/899 [==============================] - 3s 3ms/sample - loss: 4.2244\n",
      "Epoch 30/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 4.0581\n",
      "Epoch 00030: loss improved from 4.22442 to 4.06495, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 4.0649\n",
      "Epoch 31/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 3.9170\n",
      "Epoch 00031: loss improved from 4.06495 to 3.94355, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 3.9435\n",
      "Epoch 32/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 3.7752\n",
      "Epoch 00032: loss improved from 3.94355 to 3.81013, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 3.8101\n",
      "Epoch 33/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 3.6652\n",
      "Epoch 00033: loss improved from 3.81013 to 3.69176, saving model to nextword1.h5\n",
      "899/899 [==============================] - 1s 2ms/sample - loss: 3.6918\n",
      "Epoch 34/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "832/899 [==========================>...] - ETA: 0s - loss: 3.5743\n",
      "Epoch 00034: loss improved from 3.69176 to 3.61723, saving model to nextword1.h5\n",
      "899/899 [==============================] - 1s 2ms/sample - loss: 3.6172\n",
      "Epoch 35/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 3.4592\n",
      "Epoch 00035: loss improved from 3.61723 to 3.53682, saving model to nextword1.h5\n",
      "899/899 [==============================] - 1s 1ms/sample - loss: 3.5368\n",
      "Epoch 36/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 3.4625- ETA: 0s - loss: 3.2\n",
      "Epoch 00036: loss improved from 3.53682 to 3.49288, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 3.4929\n",
      "Epoch 37/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 3.4786- ETA: 0s - loss: 3.31\n",
      "Epoch 00037: loss did not improve from 3.49288\n",
      "899/899 [==============================] - 1s 781us/sample - loss: 3.5248\n",
      "Epoch 38/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 3.4306\n",
      "Epoch 00038: loss improved from 3.49288 to 3.47088, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 3.4709\n",
      "Epoch 39/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 3.3215- ETA: 0s - loss: 3.1\n",
      "Epoch 00039: loss improved from 3.47088 to 3.37350, saving model to nextword1.h5\n",
      "899/899 [==============================] - 1s 2ms/sample - loss: 3.3735\n",
      "Epoch 40/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 3.1501\n",
      "Epoch 00040: loss improved from 3.37350 to 3.19383, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 3.1938\n",
      "Epoch 41/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 3.0950\n",
      "Epoch 00041: loss improved from 3.19383 to 3.11844, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 3.1184\n",
      "Epoch 42/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 3.0236- ETA: 0s - loss: 2.8\n",
      "Epoch 00042: loss improved from 3.11844 to 3.05494, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 3.0549\n",
      "Epoch 43/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 3.0124\n",
      "Epoch 00043: loss improved from 3.05494 to 3.03522, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 3.0352\n",
      "Epoch 44/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 2.9617- ETA: 0s - loss: 2.77\n",
      "Epoch 00044: loss improved from 3.03522 to 3.00860, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 3.0086\n",
      "Epoch 45/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 3.1327\n",
      "Epoch 00045: loss did not improve from 3.00860\n",
      "899/899 [==============================] - 1s 593us/sample - loss: 3.1551\n",
      "Epoch 46/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 3.2084\n",
      "Epoch 00046: loss did not improve from 3.00860\n",
      "899/899 [==============================] - 1s 581us/sample - loss: 3.2330\n",
      "Epoch 47/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 3.0832\n",
      "Epoch 00047: loss did not improve from 3.00860\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "899/899 [==============================] - 1s 1ms/sample - loss: 3.1069\n",
      "Epoch 48/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 2.6143\n",
      "Epoch 00048: loss improved from 3.00860 to 2.59752, saving model to nextword1.h5\n",
      "899/899 [==============================] - 1s 899us/sample - loss: 2.5975\n",
      "Epoch 49/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 2.4524\n",
      "Epoch 00049: loss improved from 2.59752 to 2.45709, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 2.4571\n",
      "Epoch 50/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 2.3512\n",
      "Epoch 00050: loss improved from 2.45709 to 2.35950, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 2.3595\n",
      "Epoch 51/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 2.2831\n",
      "Epoch 00051: loss improved from 2.35950 to 2.29129, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 2.2913\n",
      "Epoch 52/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 2.2327\n",
      "Epoch 00052: loss improved from 2.29129 to 2.23967, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 2.2397\n",
      "Epoch 53/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 2.1803\n",
      "Epoch 00053: loss improved from 2.23967 to 2.18365, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 2.1836\n",
      "Epoch 54/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 2.1149\n",
      "Epoch 00054: loss improved from 2.18365 to 2.12293, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 2.1229\n",
      "Epoch 55/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 2.0594- ETA: 0s - loss: 2.0\n",
      "Epoch 00055: loss improved from 2.12293 to 2.06872, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 2.0687\n",
      "Epoch 56/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 2.0148\n",
      "Epoch 00056: loss improved from 2.06872 to 2.02338, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 2.0234\n",
      "Epoch 57/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 1.9785\n",
      "Epoch 00057: loss improved from 2.02338 to 1.98110, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 3ms/sample - loss: 1.9811\n",
      "Epoch 58/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 1.9255\n",
      "Epoch 00058: loss improved from 1.98110 to 1.94011, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 1.9401\n",
      "Epoch 59/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 1.8967\n",
      "Epoch 00059: loss improved from 1.94011 to 1.91263, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 1.9126\n",
      "Epoch 60/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 1.8724- ETA: 0s - loss: 1.82\n",
      "Epoch 00060: loss improved from 1.91263 to 1.88825, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 1.8883\n",
      "Epoch 61/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 1.8528\n",
      "Epoch 00061: loss improved from 1.88825 to 1.85042, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 1.8504\n",
      "Epoch 62/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 1.7830\n",
      "Epoch 00062: loss improved from 1.85042 to 1.79853, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 1.7985\n",
      "Epoch 63/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 1.7567\n",
      "Epoch 00063: loss improved from 1.79853 to 1.77486, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 1.7749\n",
      "Epoch 64/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 1.7304\n",
      "Epoch 00064: loss improved from 1.77486 to 1.73910, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 1.7391\n",
      "Epoch 65/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 1.7041\n",
      "Epoch 00065: loss improved from 1.73910 to 1.71630, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 1.7163\n",
      "Epoch 66/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 1.6587\n",
      "Epoch 00066: loss improved from 1.71630 to 1.67279, saving model to nextword1.h5\n",
      "899/899 [==============================] - 1s 2ms/sample - loss: 1.6728\n",
      "Epoch 67/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 1.6323\n",
      "Epoch 00067: loss improved from 1.67279 to 1.63470, saving model to nextword1.h5\n",
      "899/899 [==============================] - 1s 2ms/sample - loss: 1.6347\n",
      "Epoch 68/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 1.6229\n",
      "Epoch 00068: loss did not improve from 1.63470\n",
      "899/899 [==============================] - 1s 562us/sample - loss: 1.6460\n",
      "Epoch 69/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "832/899 [==========================>...] - ETA: 0s - loss: 1.5941\n",
      "Epoch 00069: loss improved from 1.63470 to 1.59375, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 1.5937\n",
      "Epoch 70/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 1.5439\n",
      "Epoch 00070: loss improved from 1.59375 to 1.56968, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 1.5697\n",
      "Epoch 71/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 1.5304\n",
      "Epoch 00071: loss improved from 1.56968 to 1.54010, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 1.5401\n",
      "Epoch 72/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 1.496 - ETA: 0s - loss: 1.5009\n",
      "Epoch 00072: loss improved from 1.54010 to 1.50756, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 1.5076\n",
      "Epoch 73/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 1.4789\n",
      "Epoch 00073: loss improved from 1.50756 to 1.49004, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 1.4900\n",
      "Epoch 74/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 1.4565\n",
      "Epoch 00074: loss improved from 1.49004 to 1.46704, saving model to nextword1.h5\n",
      "899/899 [==============================] - 1s 2ms/sample - loss: 1.4670\n",
      "Epoch 75/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 1.4260\n",
      "Epoch 00075: loss improved from 1.46704 to 1.45273, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 1.4527\n",
      "Epoch 76/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 1.4057\n",
      "Epoch 00076: loss improved from 1.45273 to 1.41626, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 1.4163\n",
      "Epoch 77/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 1.3743\n",
      "Epoch 00077: loss improved from 1.41626 to 1.38233, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 1.3823\n",
      "Epoch 78/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 1.3530\n",
      "Epoch 00078: loss improved from 1.38233 to 1.36472, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 1.3647\n",
      "Epoch 79/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 1.3329\n",
      "Epoch 00079: loss improved from 1.36472 to 1.33684, saving model to nextword1.h5\n",
      "899/899 [==============================] - 1s 2ms/sample - loss: 1.3368\n",
      "Epoch 80/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 1.2895\n",
      "Epoch 00080: loss improved from 1.33684 to 1.30294, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 1.3029\n",
      "Epoch 81/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 1.2538\n",
      "Epoch 00081: loss improved from 1.30294 to 1.27451, saving model to nextword1.h5\n",
      "899/899 [==============================] - 1s 2ms/sample - loss: 1.2745\n",
      "Epoch 82/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 1.2284\n",
      "Epoch 00082: loss improved from 1.27451 to 1.24444, saving model to nextword1.h5\n",
      "899/899 [==============================] - 1s 2ms/sample - loss: 1.2444\n",
      "Epoch 83/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 1.1864\n",
      "Epoch 00083: loss improved from 1.24444 to 1.20438, saving model to nextword1.h5\n",
      "899/899 [==============================] - 1s 2ms/sample - loss: 1.2044\n",
      "Epoch 84/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 1.1818\n",
      "Epoch 00084: loss improved from 1.20438 to 1.19070, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 1.1907\n",
      "Epoch 85/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 1.1668- ETA: 0s - loss: 1.11\n",
      "Epoch 00085: loss improved from 1.19070 to 1.17011, saving model to nextword1.h5\n",
      "899/899 [==============================] - 1s 2ms/sample - loss: 1.1701\n",
      "Epoch 86/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 1.1585\n",
      "Epoch 00086: loss did not improve from 1.17011\n",
      "899/899 [==============================] - 1s 677us/sample - loss: 1.1726\n",
      "Epoch 87/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 1.1336\n",
      "Epoch 00087: loss improved from 1.17011 to 1.13841, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 1.1384\n",
      "Epoch 88/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 1.0975\n",
      "Epoch 00088: loss improved from 1.13841 to 1.11198, saving model to nextword1.h5\n",
      "899/899 [==============================] - 1s 2ms/sample - loss: 1.1120\n",
      "Epoch 89/150\n",
      "896/899 [============================>.] - ETA: 0s - loss: 1.0727\n",
      "Epoch 00089: loss improved from 1.11198 to 1.07326, saving model to nextword1.h5\n",
      "899/899 [==============================] - 1s 2ms/sample - loss: 1.0733\n",
      "Epoch 90/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 1.0451\n",
      "Epoch 00090: loss improved from 1.07326 to 1.05790, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 1.0579\n",
      "Epoch 91/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 1.0044\n",
      "Epoch 00091: loss improved from 1.05790 to 1.01525, saving model to nextword1.h5\n",
      "899/899 [==============================] - 1s 2ms/sample - loss: 1.0152\n",
      "Epoch 92/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 0.9881\n",
      "Epoch 00092: loss improved from 1.01525 to 1.00014, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 1.0001\n",
      "Epoch 93/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 0.9474\n",
      "Epoch 00093: loss improved from 1.00014 to 0.95802, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 0.9580\n",
      "Epoch 94/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 0.9188\n",
      "Epoch 00094: loss improved from 0.95802 to 0.93091, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 0.9309\n",
      "Epoch 95/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 0.8827\n",
      "Epoch 00095: loss improved from 0.93091 to 0.90105, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 0.9011\n",
      "Epoch 96/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 0.8756\n",
      "Epoch 00096: loss improved from 0.90105 to 0.90065, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 0.9006\n",
      "Epoch 97/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 0.9062\n",
      "Epoch 00097: loss did not improve from 0.90065\n",
      "899/899 [==============================] - 1s 584us/sample - loss: 0.9159\n",
      "Epoch 98/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 0.8814\n",
      "Epoch 00098: loss improved from 0.90065 to 0.88223, saving model to nextword1.h5\n",
      "899/899 [==============================] - 1s 2ms/sample - loss: 0.8822\n",
      "Epoch 99/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 0.8402\n",
      "Epoch 00099: loss improved from 0.88223 to 0.85906, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 0.8591\n",
      "Epoch 100/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 0.8123\n",
      "Epoch 00100: loss improved from 0.85906 to 0.82194, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 3ms/sample - loss: 0.8219\n",
      "Epoch 101/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 0.7937- ETA: 0s - loss: 0.77\n",
      "Epoch 00101: loss improved from 0.82194 to 0.79401, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 0.7940\n",
      "Epoch 102/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 0.7795\n",
      "Epoch 00102: loss did not improve from 0.79401\n",
      "899/899 [==============================] - 1s 694us/sample - loss: 0.7963\n",
      "Epoch 103/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 0.7935\n",
      "Epoch 00103: loss did not improve from 0.79401\n",
      "899/899 [==============================] - 1s 675us/sample - loss: 0.8000\n",
      "Epoch 104/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 0.7963\n",
      "Epoch 00104: loss did not improve from 0.79401\n",
      "\n",
      "Epoch 00104: ReduceLROnPlateau reducing learning rate to 0.0001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "899/899 [==============================] - 1s 776us/sample - loss: 0.8040\n",
      "Epoch 105/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 0.7022\n",
      "Epoch 00105: loss improved from 0.79401 to 0.69619, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 0.6962\n",
      "Epoch 106/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 0.6716\n",
      "Epoch 00106: loss improved from 0.69619 to 0.67205, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 0.6721\n",
      "Epoch 107/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 0.6443\n",
      "Epoch 00107: loss improved from 0.67205 to 0.64260, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 0.6426\n",
      "Epoch 108/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 0.6149\n",
      "Epoch 00108: loss improved from 0.64260 to 0.62211, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 0.6221\n",
      "Epoch 109/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 0.6108\n",
      "Epoch 00109: loss improved from 0.62211 to 0.60798, saving model to nextword1.h5\n",
      "899/899 [==============================] - 1s 931us/sample - loss: 0.6080\n",
      "Epoch 110/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 0.5794\n",
      "Epoch 00110: loss improved from 0.60798 to 0.59935, saving model to nextword1.h5\n",
      "899/899 [==============================] - 1s 996us/sample - loss: 0.5994\n",
      "Epoch 111/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 0.562 - ETA: 0s - loss: 0.5833\n",
      "Epoch 00111: loss improved from 0.59935 to 0.58212, saving model to nextword1.h5\n",
      "899/899 [==============================] - 1s 1ms/sample - loss: 0.5821\n",
      "Epoch 112/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 0.5710\n",
      "Epoch 00112: loss improved from 0.58212 to 0.57449, saving model to nextword1.h5\n",
      "899/899 [==============================] - 1s 1ms/sample - loss: 0.5745\n",
      "Epoch 113/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 0.5510- ETA: 0s - loss: 0.542\n",
      "Epoch 00113: loss improved from 0.57449 to 0.56695, saving model to nextword1.h5\n",
      "899/899 [==============================] - 1s 1ms/sample - loss: 0.5670\n",
      "Epoch 114/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 0.5505\n",
      "Epoch 00114: loss improved from 0.56695 to 0.55014, saving model to nextword1.h5\n",
      "899/899 [==============================] - 1s 1ms/sample - loss: 0.5501\n",
      "Epoch 115/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 0.5251\n",
      "Epoch 00115: loss improved from 0.55014 to 0.53028, saving model to nextword1.h5\n",
      "899/899 [==============================] - 1s 1ms/sample - loss: 0.5303\n",
      "Epoch 116/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 0.5093\n",
      "Epoch 00116: loss did not improve from 0.53028\n",
      "899/899 [==============================] - 0s 555us/sample - loss: 0.5311\n",
      "Epoch 117/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 0.5112- ETA: 0s - loss: 0.51\n",
      "Epoch 00117: loss improved from 0.53028 to 0.51807, saving model to nextword1.h5\n",
      "899/899 [==============================] - 1s 938us/sample - loss: 0.5181\n",
      "Epoch 118/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 0.5075\n",
      "Epoch 00118: loss improved from 0.51807 to 0.50686, saving model to nextword1.h5\n",
      "899/899 [==============================] - 3s 3ms/sample - loss: 0.5069\n",
      "Epoch 119/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 0.4925\n",
      "Epoch 00119: loss improved from 0.50686 to 0.50012, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 0.5001\n",
      "Epoch 120/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 0.4730\n",
      "Epoch 00120: loss improved from 0.50012 to 0.49230, saving model to nextword1.h5\n",
      "899/899 [==============================] - 1s 2ms/sample - loss: 0.4923\n",
      "Epoch 121/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 0.4812\n",
      "Epoch 00121: loss improved from 0.49230 to 0.48230, saving model to nextword1.h5\n",
      "899/899 [==============================] - 1s 1ms/sample - loss: 0.4823\n",
      "Epoch 122/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 0.4647\n",
      "Epoch 00122: loss improved from 0.48230 to 0.46941, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 0.4694\n",
      "Epoch 123/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 0.4468\n",
      "Epoch 00123: loss improved from 0.46941 to 0.45495, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 0.4549\n",
      "Epoch 124/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 0.4436\n",
      "Epoch 00124: loss improved from 0.45495 to 0.44229, saving model to nextword1.h5\n",
      "899/899 [==============================] - 1s 1ms/sample - loss: 0.4423\n",
      "Epoch 125/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 0.4403\n",
      "Epoch 00125: loss improved from 0.44229 to 0.44091, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 0.4409\n",
      "Epoch 126/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 0.4468\n",
      "Epoch 00126: loss did not improve from 0.44091\n",
      "899/899 [==============================] - 1s 702us/sample - loss: 0.4543\n",
      "Epoch 127/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 0.4427\n",
      "Epoch 00127: loss improved from 0.44091 to 0.44014, saving model to nextword1.h5\n",
      "899/899 [==============================] - 1s 1ms/sample - loss: 0.4401\n",
      "Epoch 128/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 0.4197\n",
      "Epoch 00128: loss improved from 0.44014 to 0.41802, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 0.4180\n",
      "Epoch 129/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 0.3961\n",
      "Epoch 00129: loss improved from 0.41802 to 0.40157, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 0.4016\n",
      "Epoch 130/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 0.3982\n",
      "Epoch 00130: loss improved from 0.40157 to 0.39946, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 0.3995\n",
      "Epoch 131/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 0.4020\n",
      "Epoch 00131: loss did not improve from 0.39946\n",
      "899/899 [==============================] - 1s 850us/sample - loss: 0.4055\n",
      "Epoch 132/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 0.3989\n",
      "Epoch 00132: loss improved from 0.39946 to 0.39939, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 0.3994\n",
      "Epoch 133/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 0.3888\n",
      "Epoch 00133: loss improved from 0.39939 to 0.39490, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 0.3949\n",
      "Epoch 134/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 0.3768\n",
      "Epoch 00134: loss improved from 0.39490 to 0.38597, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 0.3860\n",
      "Epoch 135/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 0.3736\n",
      "Epoch 00135: loss improved from 0.38597 to 0.37741, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 0.3774\n",
      "Epoch 136/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 0.3739\n",
      "Epoch 00136: loss improved from 0.37741 to 0.37421, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 0.3742\n",
      "Epoch 137/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 0.3920\n",
      "Epoch 00137: loss did not improve from 0.37421\n",
      "899/899 [==============================] - 1s 759us/sample - loss: 0.3928\n",
      "Epoch 138/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 0.3932\n",
      "Epoch 00138: loss did not improve from 0.37421\n",
      "899/899 [==============================] - 1s 602us/sample - loss: 0.3940\n",
      "Epoch 139/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 0.3946\n",
      "Epoch 00139: loss did not improve from 0.37421\n",
      "899/899 [==============================] - 1s 752us/sample - loss: 0.3914\n",
      "Epoch 140/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 0.3726\n",
      "Epoch 00140: loss did not improve from 0.37421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "899/899 [==============================] - 1s 768us/sample - loss: 0.3823\n",
      "Epoch 141/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 0.3676\n",
      "Epoch 00141: loss did not improve from 0.37421\n",
      "899/899 [==============================] - 1s 859us/sample - loss: 0.3811\n",
      "Epoch 142/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 0.3911\n",
      "Epoch 00142: loss did not improve from 0.37421\n",
      "899/899 [==============================] - 1s 714us/sample - loss: 0.3903\n",
      "Epoch 143/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 0.3833\n",
      "Epoch 00143: loss did not improve from 0.37421\n",
      "899/899 [==============================] - 1s 780us/sample - loss: 0.3810\n",
      "Epoch 144/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 0.3582\n",
      "Epoch 00144: loss improved from 0.37421 to 0.37420, saving model to nextword1.h5\n",
      "899/899 [==============================] - 1s 2ms/sample - loss: 0.3742\n",
      "Epoch 145/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 0.3724\n",
      "Epoch 00145: loss improved from 0.37420 to 0.36889, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 0.3689\n",
      "Epoch 146/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 0.3544\n",
      "Epoch 00146: loss improved from 0.36889 to 0.36289, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 0.3629\n",
      "Epoch 147/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 0.3477\n",
      "Epoch 00147: loss improved from 0.36289 to 0.35336, saving model to nextword1.h5\n",
      "899/899 [==============================] - 2s 2ms/sample - loss: 0.3534\n",
      "Epoch 148/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 0.3663- ETA: 0s - loss: 0.3\n",
      "Epoch 00148: loss did not improve from 0.35336\n",
      "899/899 [==============================] - 1s 799us/sample - loss: 0.3675\n",
      "Epoch 149/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 0.3710\n",
      "Epoch 00149: loss did not improve from 0.35336\n",
      "899/899 [==============================] - 1s 768us/sample - loss: 0.3798\n",
      "Epoch 150/150\n",
      "832/899 [==========================>...] - ETA: 0s - loss: 0.3652\n",
      "Epoch 00150: loss did not improve from 0.35336\n",
      "899/899 [==============================] - 1s 775us/sample - loss: 0.3711\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x196739219c8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=150, batch_size=64, callbacks=[checkpoint, reduce, tensorboard_Visualization])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2ce8c7d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'graph1.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16040/2863566802.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpil_img\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'graph1.png'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpil_img\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\lib\\site-packages\\IPython\\core\\display.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, url, filename, format, embed, width, height, retina, unconfined, metadata)\u001b[0m\n\u001b[0;32m   1230\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munconfined\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munconfined\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1231\u001b[0m         super(Image, self).__init__(data=data, url=url, filename=filename, \n\u001b[1;32m-> 1232\u001b[1;33m                 metadata=metadata)\n\u001b[0m\u001b[0;32m   1233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1234\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwidth\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'width'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\lib\\site-packages\\IPython\\core\\display.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, url, filename, metadata)\u001b[0m\n\u001b[0;32m    635\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 637\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    638\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    639\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\lib\\site-packages\\IPython\\core\\display.py\u001b[0m in \u001b[0;36mreload\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1261\u001b[0m         \u001b[1;34m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1262\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1263\u001b[1;33m             \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1264\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretina\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1265\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_retina_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\lib\\site-packages\\IPython\\core\\display.py\u001b[0m in \u001b[0;36mreload\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    660\u001b[0m         \u001b[1;34m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    661\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 662\u001b[1;33m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_flags\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    663\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    664\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murl\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'graph1.png'"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image \n",
    "pil_img = Image(filename='graph1.png')\n",
    "display(pil_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df9aa08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
