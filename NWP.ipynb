{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff1e5f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Suleyman\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Suleyman\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Suleyman\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Suleyman\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Suleyman\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Suleyman\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2a44a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"dt.txt\", \"r\", encoding = \"utf8\")\n",
    "lines = []\n",
    "\n",
    "for i in file:\n",
    "    lines.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8737817b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"\"\n",
    "\n",
    "for i in lines:\n",
    "    data = ' '. join(lines)\n",
    "    \n",
    "data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce0b72df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) #map punctuation to space\n",
    "new_data = data.translate(translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a98eadae",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = []\n",
    "\n",
    "for i in data.split():\n",
    "    if i not in z:\n",
    "        z.append(i)\n",
    "        \n",
    "data = ' '.join(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b3d633c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([data])\n",
    "\n",
    "pickle.dump(tokenizer, open('tokenizer1.pkl', 'wb'))\n",
    "\n",
    "sequence_data = tokenizer.texts_to_sequences([data])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b964af3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e5642e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = []\n",
    "\n",
    "for i in range(1, len(sequence_data)):\n",
    "    words = sequence_data[i-1:i+1]\n",
    "    sequences.append(words)\n",
    "    \n",
    "sequences = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15beda3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in sequences:\n",
    "    X.append(i[0])\n",
    "    y.append(i[1])\n",
    "    \n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fce7c22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(y, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eb181e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=1))\n",
    "model.add(LSTM(1000, return_sequences=True))\n",
    "model.add(LSTM(1000))\n",
    "model.add(Dense(1000, activation=\"relu\"))\n",
    "model.add(Dense(vocab_size, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "289f7ced",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "keras.utils.plot_model(model, to_file='model.png', show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "07527aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.python.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"nextword1.h5\", monitor='loss', verbose=1,\n",
    "    save_best_only=True, mode='auto')\n",
    "\n",
    "reduce = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.0001, verbose = 1)\n",
    "\n",
    "logdir='logsnextword1'\n",
    "tensorboard_Visualization = TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b8e0d624",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2df9aa08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.772 - ETA: 0s - loss: 0.7981\n",
      "Epoch 00001: loss improved from 0.81551 to 0.80974, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 2s 2ms/sample - loss: 0.8097\n",
      "Epoch 2/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.7836\n",
      "Epoch 00002: loss improved from 0.80974 to 0.79982, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 1s 974us/sample - loss: 0.7998\n",
      "Epoch 3/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.7619- ETA: 0s - loss: 0.677 - ETA: 0s - loss: 0.7\n",
      "Epoch 00003: loss improved from 0.79982 to 0.77852, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 2s 1ms/sample - loss: 0.7785\n",
      "Epoch 4/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.7639- ETA: 0s - loss: 0.\n",
      "Epoch 00004: loss improved from 0.77852 to 0.77558, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 1s 982us/sample - loss: 0.7756\n",
      "Epoch 5/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.7680\n",
      "Epoch 00005: loss improved from 0.77558 to 0.77450, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 2s 1ms/sample - loss: 0.7745\n",
      "Epoch 6/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.7670- ETA: 0s - loss: 0\n",
      "Epoch 00006: loss improved from 0.77450 to 0.76910, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 2s 2ms/sample - loss: 0.7691\n",
      "Epoch 7/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.7615\n",
      "Epoch 00007: loss did not improve from 0.76910\n",
      "1266/1266 [==============================] - 1s 508us/sample - loss: 0.7730\n",
      "Epoch 8/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.733 - ETA: 0s - loss: 0.7763\n",
      "Epoch 00008: loss did not improve from 0.76910\n",
      "1266/1266 [==============================] - 1s 544us/sample - loss: 0.7852\n",
      "Epoch 9/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.7736\n",
      "Epoch 00009: loss did not improve from 0.76910\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
      "1266/1266 [==============================] - 1s 508us/sample - loss: 0.7825\n",
      "Epoch 10/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.6511- ETA: 0s - loss: 0.641\n",
      "Epoch 00010: loss improved from 0.76910 to 0.65566, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 2s 2ms/sample - loss: 0.6557\n",
      "Epoch 11/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.6220\n",
      "Epoch 00011: loss improved from 0.65566 to 0.62341, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 2s 2ms/sample - loss: 0.6234\n",
      "Epoch 12/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.6029\n",
      "Epoch 00012: loss improved from 0.62341 to 0.61217, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 2s 1ms/sample - loss: 0.6122\n",
      "Epoch 13/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.6038\n",
      "Epoch 00013: loss improved from 0.61217 to 0.60717, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 2s 2ms/sample - loss: 0.6072\n",
      "Epoch 14/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.5999\n",
      "Epoch 00014: loss improved from 0.60717 to 0.60005, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 1s 859us/sample - loss: 0.6001\n",
      "Epoch 15/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.5996- ETA: 0s - loss: 0.5\n",
      "Epoch 00015: loss did not improve from 0.60005\n",
      "1266/1266 [==============================] - 1s 514us/sample - loss: 0.6020\n",
      "Epoch 16/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.5944- ETA: 0s - loss: 0.58\n",
      "Epoch 00016: loss improved from 0.60005 to 0.59801, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 2s 1ms/sample - loss: 0.5980\n",
      "Epoch 17/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.5873\n",
      "Epoch 00017: loss improved from 0.59801 to 0.59065, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 3s 2ms/sample - loss: 0.5906\n",
      "Epoch 18/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.5906\n",
      "Epoch 00018: loss improved from 0.59065 to 0.59054, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 2s 2ms/sample - loss: 0.5905\n",
      "Epoch 19/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.5791- ETA: 0s - loss: 0.569\n",
      "Epoch 00019: loss improved from 0.59054 to 0.58640, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 2s 1ms/sample - loss: 0.5864\n",
      "Epoch 20/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.5792\n",
      "Epoch 00020: loss did not improve from 0.58640\n",
      "1266/1266 [==============================] - 1s 512us/sample - loss: 0.5865\n",
      "Epoch 21/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.5764- ETA: 0s - loss: 0\n",
      "Epoch 00021: loss did not improve from 0.58640\n",
      "1266/1266 [==============================] - 1s 526us/sample - loss: 0.5869\n",
      "Epoch 22/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.5787- ETA: 0s - loss: - ETA: 0s - loss: 0.567\n",
      "Epoch 00022: loss improved from 0.58640 to 0.58198, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 1s 876us/sample - loss: 0.5820\n",
      "Epoch 23/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.5732\n",
      "Epoch 00023: loss improved from 0.58198 to 0.57839, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 1s 929us/sample - loss: 0.5784\n",
      "Epoch 24/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.5783\n",
      "Epoch 00024: loss did not improve from 0.57839\n",
      "1266/1266 [==============================] - 1s 508us/sample - loss: 0.5836\n",
      "Epoch 25/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.5698- ETA: 0s - loss: 0.556\n",
      "Epoch 00025: loss improved from 0.57839 to 0.57535, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 1s 921us/sample - loss: 0.5753\n",
      "Epoch 26/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.5740\n",
      "Epoch 00026: loss improved from 0.57535 to 0.57385, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 1s 973us/sample - loss: 0.5738\n",
      "Epoch 27/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.5594\n",
      "Epoch 00027: loss improved from 0.57385 to 0.57050, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 2s 1ms/sample - loss: 0.5705\n",
      "Epoch 28/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.5656\n",
      "Epoch 00028: loss did not improve from 0.57050\n",
      "1266/1266 [==============================] - 1s 652us/sample - loss: 0.5721\n",
      "Epoch 29/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.5644\n",
      "Epoch 00029: loss improved from 0.57050 to 0.56652, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 2s 2ms/sample - loss: 0.5665\n",
      "Epoch 30/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.5585\n",
      "Epoch 00030: loss improved from 0.56652 to 0.56383, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 2s 2ms/sample - loss: 0.5638\n",
      "Epoch 31/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.523 - ETA: 0s - loss: 0.527 - ETA: 0s - loss: 0.534 - ETA: 0s - loss: 0.5533\n",
      "Epoch 00031: loss improved from 0.56383 to 0.55972, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 2s 1ms/sample - loss: 0.5597\n",
      "Epoch 32/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.5577\n",
      "Epoch 00032: loss did not improve from 0.55972\n",
      "1266/1266 [==============================] - 1s 718us/sample - loss: 0.5624\n",
      "Epoch 33/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.537 - ETA: 0s - loss: 0.5603\n",
      "Epoch 00033: loss improved from 0.55972 to 0.55906, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 2s 2ms/sample - loss: 0.5591\n",
      "Epoch 34/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.5446- ETA: 0s - loss: 0.5\n",
      "Epoch 00034: loss improved from 0.55906 to 0.55050, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 1s 1ms/sample - loss: 0.5505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.5466\n",
      "Epoch 00035: loss did not improve from 0.55050\n",
      "1266/1266 [==============================] - 1s 613us/sample - loss: 0.5530\n",
      "Epoch 36/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.5411\n",
      "Epoch 00036: loss improved from 0.55050 to 0.54494, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 2s 1ms/sample - loss: 0.5449\n",
      "Epoch 37/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.5463\n",
      "Epoch 00037: loss did not improve from 0.54494\n",
      "1266/1266 [==============================] - 1s 681us/sample - loss: 0.5505\n",
      "Epoch 38/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.5471\n",
      "Epoch 00038: loss did not improve from 0.54494\n",
      "1266/1266 [==============================] - 1s 548us/sample - loss: 0.5506\n",
      "Epoch 39/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.5399\n",
      "Epoch 00039: loss improved from 0.54494 to 0.54178, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 2s 2ms/sample - loss: 0.5418\n",
      "Epoch 40/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.5366\n",
      "Epoch 00040: loss did not improve from 0.54178\n",
      "1266/1266 [==============================] - 1s 636us/sample - loss: 0.5443\n",
      "Epoch 41/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.5344- ETA: 0s - loss: 0.490 - ETA: 0s - loss: 0.5\n",
      "Epoch 00041: loss improved from 0.54178 to 0.53778, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 2s 2ms/sample - loss: 0.5378\n",
      "Epoch 42/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.5334\n",
      "Epoch 00042: loss improved from 0.53778 to 0.53775, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 1s 978us/sample - loss: 0.5377\n",
      "Epoch 43/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.5299\n",
      "Epoch 00043: loss improved from 0.53775 to 0.53365, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 1s 1ms/sample - loss: 0.5337\n",
      "Epoch 44/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.5285- ETA: 0s - loss: 0\n",
      "Epoch 00044: loss improved from 0.53365 to 0.53296, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 2s 1ms/sample - loss: 0.5330\n",
      "Epoch 45/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.5295\n",
      "Epoch 00045: loss improved from 0.53296 to 0.53088, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 2s 1ms/sample - loss: 0.5309\n",
      "Epoch 46/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.5231\n",
      "Epoch 00046: loss improved from 0.53088 to 0.52511, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 2s 2ms/sample - loss: 0.5251\n",
      "Epoch 47/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.5251\n",
      "Epoch 00047: loss did not improve from 0.52511\n",
      "1266/1266 [==============================] - 1s 626us/sample - loss: 0.5296\n",
      "Epoch 48/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.5239\n",
      "Epoch 00048: loss did not improve from 0.52511\n",
      "1266/1266 [==============================] - 1s 619us/sample - loss: 0.5259\n",
      "Epoch 49/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.5162\n",
      "Epoch 00049: loss improved from 0.52511 to 0.52360, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 2s 1ms/sample - loss: 0.5236\n",
      "Epoch 50/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.5181- ETA: 0s - loss: 0.471 - ETA: 0s - loss: 0.\n",
      "Epoch 00050: loss improved from 0.52360 to 0.51855, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 2s 1ms/sample - loss: 0.5185\n",
      "Epoch 51/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.5095- ETA: 0s - loss: 0.49\n",
      "Epoch 00051: loss did not improve from 0.51855\n",
      "1266/1266 [==============================] - 1s 509us/sample - loss: 0.5189\n",
      "Epoch 52/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.5136\n",
      "Epoch 00052: loss improved from 0.51855 to 0.51850, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 1s 982us/sample - loss: 0.5185\n",
      "Epoch 53/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.5180\n",
      "Epoch 00053: loss improved from 0.51850 to 0.51763, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 1s 824us/sample - loss: 0.5176\n",
      "Epoch 54/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.5156- ETA: 0s - loss: 0.4\n",
      "Epoch 00054: loss improved from 0.51763 to 0.51505, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 2s 1ms/sample - loss: 0.5150\n",
      "Epoch 55/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.5038\n",
      "Epoch 00055: loss improved from 0.51505 to 0.50429, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 1s 1ms/sample - loss: 0.5043\n",
      "Epoch 56/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.5004\n",
      "Epoch 00056: loss did not improve from 0.50429\n",
      "1266/1266 [==============================] - 1s 613us/sample - loss: 0.5048\n",
      "Epoch 57/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.5020\n",
      "Epoch 00057: loss did not improve from 0.50429\n",
      "1266/1266 [==============================] - 1s 525us/sample - loss: 0.5061\n",
      "Epoch 58/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.4972- ETA: 0s - loss\n",
      "Epoch 00058: loss improved from 0.50429 to 0.50026, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 2s 2ms/sample - loss: 0.5003\n",
      "Epoch 59/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.4985- ETA: 0s - loss: 0.\n",
      "Epoch 00059: loss did not improve from 0.50026\n",
      "1266/1266 [==============================] - 1s 857us/sample - loss: 0.5021\n",
      "Epoch 60/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.4979\n",
      "Epoch 00060: loss did not improve from 0.50026\n",
      "1266/1266 [==============================] - 1s 688us/sample - loss: 0.5019\n",
      "Epoch 61/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.4958\n",
      "Epoch 00061: loss improved from 0.50026 to 0.49606, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 2s 1ms/sample - loss: 0.4961\n",
      "Epoch 62/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.4894\n",
      "Epoch 00062: loss improved from 0.49606 to 0.49582, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 2s 1ms/sample - loss: 0.4958\n",
      "Epoch 63/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.4919\n",
      "Epoch 00063: loss improved from 0.49582 to 0.49262, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 2s 1ms/sample - loss: 0.4926\n",
      "Epoch 64/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.4851\n",
      "Epoch 00064: loss improved from 0.49262 to 0.48933, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 2s 1ms/sample - loss: 0.4893\n",
      "Epoch 65/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.475 - ETA: 0s - loss: 0.4860\n",
      "Epoch 00065: loss improved from 0.48933 to 0.48786, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 2s 1ms/sample - loss: 0.4879\n",
      "Epoch 66/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.4789\n",
      "Epoch 00066: loss did not improve from 0.48786\n",
      "1266/1266 [==============================] - 1s 684us/sample - loss: 0.4894\n",
      "Epoch 67/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.4841\n",
      "Epoch 00067: loss improved from 0.48786 to 0.48690, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 2s 1ms/sample - loss: 0.4869\n",
      "Epoch 68/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.4848\n",
      "Epoch 00068: loss did not improve from 0.48690\n",
      "1266/1266 [==============================] - 1s 812us/sample - loss: 0.4900\n",
      "Epoch 69/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.4835\n",
      "Epoch 00069: loss improved from 0.48690 to 0.48621, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 2s 1ms/sample - loss: 0.4862\n",
      "Epoch 70/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.4773\n",
      "Epoch 00070: loss improved from 0.48621 to 0.48091, saving model to nextword1.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1266/1266 [==============================] - 2s 1ms/sample - loss: 0.4809\n",
      "Epoch 71/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.4820\n",
      "Epoch 00071: loss did not improve from 0.48091\n",
      "1266/1266 [==============================] - 1s 651us/sample - loss: 0.4844\n",
      "Epoch 72/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.4705\n",
      "Epoch 00072: loss improved from 0.48091 to 0.47687, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 2s 2ms/sample - loss: 0.4769\n",
      "Epoch 73/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.4707\n",
      "Epoch 00073: loss did not improve from 0.47687\n",
      "1266/1266 [==============================] - 1s 539us/sample - loss: 0.4797\n",
      "Epoch 74/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.4737\n",
      "Epoch 00074: loss improved from 0.47687 to 0.47681, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 2s 2ms/sample - loss: 0.4768\n",
      "Epoch 75/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.4674\n",
      "Epoch 00075: loss improved from 0.47681 to 0.47065, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 2s 1ms/sample - loss: 0.4707\n",
      "Epoch 76/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.4609- ETA: 0s - loss: 0.\n",
      "Epoch 00076: loss did not improve from 0.47065\n",
      "1266/1266 [==============================] - 1s 638us/sample - loss: 0.4711\n",
      "Epoch 77/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.4555\n",
      "Epoch 00077: loss improved from 0.47065 to 0.46635, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 1s 911us/sample - loss: 0.4663\n",
      "Epoch 78/150\n",
      "1152/1266 [==========================>...] - ETA: 0s - loss: 0.4629\n",
      "Epoch 00078: loss did not improve from 0.46635\n",
      "1266/1266 [==============================] - 1s 661us/sample - loss: 0.4682\n",
      "Epoch 79/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.4606\n",
      "Epoch 00079: loss improved from 0.46635 to 0.46556, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 2s 1ms/sample - loss: 0.4656\n",
      "Epoch 80/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.4635- ETA: 0s - loss: 0\n",
      "Epoch 00080: loss did not improve from 0.46556\n",
      "1266/1266 [==============================] - 1s 839us/sample - loss: 0.4708\n",
      "Epoch 81/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.4535\n",
      "Epoch 00081: loss improved from 0.46556 to 0.45983, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 2s 2ms/sample - loss: 0.4598\n",
      "Epoch 82/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.4596\n",
      "Epoch 00082: loss did not improve from 0.45983\n",
      "1266/1266 [==============================] - 1s 620us/sample - loss: 0.4617\n",
      "Epoch 83/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.4580\n",
      "Epoch 00083: loss did not improve from 0.45983\n",
      "1266/1266 [==============================] - 1s 827us/sample - loss: 0.4611\n",
      "Epoch 84/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.4455- ETA: 0s - loss: 0 - ETA: 0s - loss: 0.4586\n",
      "Epoch 00084: loss improved from 0.45983 to 0.45730, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 2s 2ms/sample - loss: 0.4573\n",
      "Epoch 85/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.4484\n",
      "Epoch 00085: loss improved from 0.45730 to 0.45728, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 2s 1ms/sample - loss: 0.4573\n",
      "Epoch 86/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.4455\n",
      "Epoch 00086: loss improved from 0.45728 to 0.44986, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 2s 1ms/sample - loss: 0.4499\n",
      "Epoch 87/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.4532\n",
      "Epoch 00087: loss did not improve from 0.44986\n",
      "1266/1266 [==============================] - 1s 639us/sample - loss: 0.4574\n",
      "Epoch 88/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.4509\n",
      "Epoch 00088: loss did not improve from 0.44986\n",
      "1266/1266 [==============================] - 1s 629us/sample - loss: 0.4532\n",
      "Epoch 89/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.445 - ETA: 0s - loss: 0.4554\n",
      "Epoch 00089: loss did not improve from 0.44986\n",
      "1266/1266 [==============================] - 1s 746us/sample - loss: 0.4535\n",
      "Epoch 90/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.4391\n",
      "Epoch 00090: loss did not improve from 0.44986\n",
      "1266/1266 [==============================] - 1s 607us/sample - loss: 0.4500\n",
      "Epoch 91/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.4427\n",
      "Epoch 00091: loss improved from 0.44986 to 0.44880, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 1s 772us/sample - loss: 0.4488\n",
      "Epoch 92/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.4424- ETA: 0s - loss: 0.39 - ETA: 0s - loss: 0.42\n",
      "Epoch 00092: loss did not improve from 0.44880\n",
      "1266/1266 [==============================] - 1s 538us/sample - loss: 0.4497\n",
      "Epoch 93/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.4458- ETA: 0s - loss: 0.\n",
      "Epoch 00093: loss improved from 0.44880 to 0.44859, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 2s 2ms/sample - loss: 0.4486\n",
      "Epoch 94/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.424 - ETA: 0s - loss: 0.4432\n",
      "Epoch 00094: loss improved from 0.44859 to 0.44324, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 1s 833us/sample - loss: 0.4432\n",
      "Epoch 95/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.4368\n",
      "Epoch 00095: loss improved from 0.44324 to 0.44106, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 1s 982us/sample - loss: 0.4411\n",
      "Epoch 96/150\n",
      "1152/1266 [==========================>...] - ETA: 0s - loss: 0.4297\n",
      "Epoch 00096: loss improved from 0.44106 to 0.43870, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 2s 1ms/sample - loss: 0.4387\n",
      "Epoch 97/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.4256- ETA: 0s - loss: 0.393 - ETA: 0s - loss: 0.3\n",
      "Epoch 00097: loss improved from 0.43870 to 0.43368, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 1s 938us/sample - loss: 0.4337\n",
      "Epoch 98/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.4307\n",
      "Epoch 00098: loss improved from 0.43368 to 0.43193, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 1s 931us/sample - loss: 0.4319\n",
      "Epoch 99/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.4341\n",
      "Epoch 00099: loss did not improve from 0.43193\n",
      "1266/1266 [==============================] - 1s 558us/sample - loss: 0.4341\n",
      "Epoch 100/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.4264\n",
      "Epoch 00100: loss did not improve from 0.43193\n",
      "1266/1266 [==============================] - 1s 525us/sample - loss: 0.4320\n",
      "Epoch 101/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.4274\n",
      "Epoch 00101: loss improved from 0.43193 to 0.43008, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 1s 929us/sample - loss: 0.4301\n",
      "Epoch 102/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.4216\n",
      "Epoch 00102: loss improved from 0.43008 to 0.42742, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 1s 973us/sample - loss: 0.4274\n",
      "Epoch 103/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.4287- ETA: 0s - loss: 0.\n",
      "Epoch 00103: loss did not improve from 0.42742\n",
      "1266/1266 [==============================] - 1s 767us/sample - loss: 0.4279\n",
      "Epoch 104/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.4150\n",
      "Epoch 00104: loss improved from 0.42742 to 0.42280, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 2s 2ms/sample - loss: 0.4228\n",
      "Epoch 105/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.4240\n",
      "Epoch 00105: loss did not improve from 0.42280\n",
      "1266/1266 [==============================] - 1s 679us/sample - loss: 0.4258\n",
      "Epoch 106/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.4159\n",
      "Epoch 00106: loss did not improve from 0.42280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1266/1266 [==============================] - 1s 686us/sample - loss: 0.4254\n",
      "Epoch 107/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.4216\n",
      "Epoch 00107: loss did not improve from 0.42280\n",
      "1266/1266 [==============================] - 1s 781us/sample - loss: 0.4295\n",
      "Epoch 108/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.4191- ETA: 0s - loss: 0.39\n",
      "Epoch 00108: loss did not improve from 0.42280\n",
      "1266/1266 [==============================] - 1s 716us/sample - loss: 0.4233\n",
      "Epoch 109/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.4136\n",
      "Epoch 00109: loss improved from 0.42280 to 0.41775, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 1s 833us/sample - loss: 0.4178\n",
      "Epoch 110/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.4159- ETA: 0s - loss: 0.3 - ETA: 0s - loss: 0.38\n",
      "Epoch 00110: loss did not improve from 0.41775\n",
      "1266/1266 [==============================] - 1s 541us/sample - loss: 0.4225\n",
      "Epoch 111/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.4132\n",
      "Epoch 00111: loss improved from 0.41775 to 0.41523, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 1s 965us/sample - loss: 0.4152\n",
      "Epoch 112/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.4167- ETA: 0s - loss: 0.\n",
      "Epoch 00112: loss did not improve from 0.41523\n",
      "1266/1266 [==============================] - 1s 570us/sample - loss: 0.4182\n",
      "Epoch 113/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.4092\n",
      "Epoch 00113: loss did not improve from 0.41523\n",
      "1266/1266 [==============================] - 1s 561us/sample - loss: 0.4158\n",
      "Epoch 114/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.4098\n",
      "Epoch 00114: loss improved from 0.41523 to 0.40910, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 1s 955us/sample - loss: 0.4091\n",
      "Epoch 115/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.4048- ETA: 0s - loss: 0.\n",
      "Epoch 00115: loss improved from 0.40910 to 0.40657, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 1s 893us/sample - loss: 0.4066\n",
      "Epoch 116/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.4023\n",
      "Epoch 00116: loss did not improve from 0.40657\n",
      "1266/1266 [==============================] - 1s 559us/sample - loss: 0.4069\n",
      "Epoch 117/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.4044\n",
      "Epoch 00117: loss did not improve from 0.40657\n",
      "1266/1266 [==============================] - 1s 639us/sample - loss: 0.4074\n",
      "Epoch 118/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.4032- ETA: 0s - loss: 0.390\n",
      "Epoch 00118: loss did not improve from 0.40657\n",
      "1266/1266 [==============================] - 1s 623us/sample - loss: 0.4073\n",
      "Epoch 119/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.4064- ETA: 0s - loss: 0\n",
      "Epoch 00119: loss did not improve from 0.40657\n",
      "1266/1266 [==============================] - 1s 516us/sample - loss: 0.4068\n",
      "Epoch 120/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.3988- ETA: 0s - loss:\n",
      "Epoch 00120: loss improved from 0.40657 to 0.40284, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 1s 842us/sample - loss: 0.4028\n",
      "Epoch 121/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.3940\n",
      "Epoch 00121: loss improved from 0.40284 to 0.39773, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 2s 2ms/sample - loss: 0.3977\n",
      "Epoch 122/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.3967\n",
      "Epoch 00122: loss did not improve from 0.39773\n",
      "1266/1266 [==============================] - 1s 664us/sample - loss: 0.4013\n",
      "Epoch 123/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.3995- ETA: 0s - loss: 0.\n",
      "Epoch 00123: loss did not improve from 0.39773\n",
      "1266/1266 [==============================] - 1s 638us/sample - loss: 0.4008\n",
      "Epoch 124/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.3951\n",
      "Epoch 00124: loss did not improve from 0.39773\n",
      "1266/1266 [==============================] - 1s 649us/sample - loss: 0.4027\n",
      "Epoch 125/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.3924\n",
      "Epoch 00125: loss did not improve from 0.39773\n",
      "1266/1266 [==============================] - 1s 655us/sample - loss: 0.3996\n",
      "Epoch 126/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.3888\n",
      "Epoch 00126: loss improved from 0.39773 to 0.39271, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 1s 795us/sample - loss: 0.3927\n",
      "Epoch 127/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.3897\n",
      "Epoch 00127: loss did not improve from 0.39271\n",
      "1266/1266 [==============================] - 1s 821us/sample - loss: 0.3937\n",
      "Epoch 128/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.390 - ETA: 0s - loss: 0.3946\n",
      "Epoch 00128: loss did not improve from 0.39271\n",
      "1266/1266 [==============================] - 1s 690us/sample - loss: 0.3995\n",
      "Epoch 129/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.3937\n",
      "Epoch 00129: loss did not improve from 0.39271\n",
      "1266/1266 [==============================] - 1s 796us/sample - loss: 0.3977\n",
      "Epoch 130/150\n",
      "1152/1266 [==========================>...] - ETA: 0s - loss: 0.3894\n",
      "Epoch 00130: loss did not improve from 0.39271\n",
      "1266/1266 [==============================] - 1s 608us/sample - loss: 0.3985\n",
      "Epoch 131/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.3804\n",
      "Epoch 00131: loss improved from 0.39271 to 0.38500, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 1s 667us/sample - loss: 0.3850\n",
      "Epoch 132/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.3814\n",
      "Epoch 00132: loss did not improve from 0.38500\n",
      "1266/1266 [==============================] - 1s 666us/sample - loss: 0.3854\n",
      "Epoch 133/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.3867- ETA: 0s - loss: 0.3\n",
      "Epoch 00133: loss did not improve from 0.38500\n",
      "1266/1266 [==============================] - 1s 551us/sample - loss: 0.3871\n",
      "Epoch 134/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.3844- ETA: 0s - loss: 0.\n",
      "Epoch 00134: loss improved from 0.38500 to 0.38381, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 1s 885us/sample - loss: 0.3838\n",
      "Epoch 135/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.3817\n",
      "Epoch 00135: loss improved from 0.38381 to 0.38349, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 1s 947us/sample - loss: 0.3835\n",
      "Epoch 136/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.3783\n",
      "Epoch 00136: loss did not improve from 0.38349\n",
      "1266/1266 [==============================] - 1s 518us/sample - loss: 0.3854\n",
      "Epoch 137/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.360 - ETA: 0s - loss: 0.3800\n",
      "Epoch 00137: loss did not improve from 0.38349\n",
      "1266/1266 [==============================] - 1s 561us/sample - loss: 0.3835\n",
      "Epoch 138/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.3797- ETA: 0s - loss\n",
      "Epoch 00138: loss improved from 0.38349 to 0.37927, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 1s 859us/sample - loss: 0.3793\n",
      "Epoch 139/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.3693\n",
      "Epoch 00139: loss improved from 0.37927 to 0.37507, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 1s 1ms/sample - loss: 0.3751\n",
      "Epoch 140/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.3728- ETA: 0s - loss: 0.369\n",
      "Epoch 00140: loss improved from 0.37507 to 0.37463, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 1s 1ms/sample - loss: 0.3746\n",
      "Epoch 141/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.3766\n",
      "Epoch 00141: loss did not improve from 0.37463\n",
      "1266/1266 [==============================] - 1s 488us/sample - loss: 0.3828\n",
      "Epoch 142/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.3735\n",
      "Epoch 00142: loss did not improve from 0.37463\n",
      "1266/1266 [==============================] - 1s 516us/sample - loss: 0.3782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 143/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.3739\n",
      "Epoch 00143: loss did not improve from 0.37463\n",
      "1266/1266 [==============================] - 1s 509us/sample - loss: 0.3772\n",
      "Epoch 144/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.3704\n",
      "Epoch 00144: loss did not improve from 0.37463\n",
      "1266/1266 [==============================] - 1s 552us/sample - loss: 0.3765\n",
      "Epoch 145/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.3758\n",
      "Epoch 00145: loss did not improve from 0.37463\n",
      "1266/1266 [==============================] - 1s 579us/sample - loss: 0.3770\n",
      "Epoch 146/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.3742\n",
      "Epoch 00146: loss did not improve from 0.37463\n",
      "1266/1266 [==============================] - 1s 586us/sample - loss: 0.3765\n",
      "Epoch 147/150\n",
      "1152/1266 [==========================>...] - ETA: 0s - loss: 0.3665- ETA: 0s - loss: 0.\n",
      "Epoch 00147: loss did not improve from 0.37463\n",
      "1266/1266 [==============================] - 1s 554us/sample - loss: 0.3771\n",
      "Epoch 148/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.3668\n",
      "Epoch 00148: loss improved from 0.37463 to 0.37017, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 1s 875us/sample - loss: 0.3702\n",
      "Epoch 149/150\n",
      "1152/1266 [==========================>...] - ETA: 0s - loss: 0.3710\n",
      "Epoch 00149: loss did not improve from 0.37017\n",
      "1266/1266 [==============================] - 1s 623us/sample - loss: 0.3751\n",
      "Epoch 150/150\n",
      "1216/1266 [===========================>..] - ETA: 0s - loss: 0.3641\n",
      "Epoch 00150: loss improved from 0.37017 to 0.36793, saving model to nextword1.h5\n",
      "1266/1266 [==============================] - 1s 903us/sample - loss: 0.3679\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d61b979688>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=150, batch_size=64, callbacks=[checkpoint, reduce, tensorboard_Visualization])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c893af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
